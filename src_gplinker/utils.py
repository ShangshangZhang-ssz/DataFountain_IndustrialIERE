import os
import time
import json
import random
import warnings
from pathlib import Path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

import torch
import torch.nn as nn
from torch.utils.data import random_split
from torch.nn.utils.rnn import pad_sequence
from torch.utils.data import Dataset, DataLoader, Subset

def set_seed(seed):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
        torch.backends.cudnn.deterministic = True
        torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)

class FGM():
    """
    å®šä¹‰å¯¹æŠ—è®­ç»ƒ FGM ç±»
    """
    def __init__(self, model):
        self.model = model
        self.backup = {}

    def attack(self, epsilon=1.0, emb_name='word_embeddings'):
        """
        å¯¹ Embedding å±‚æ³¨å…¥æ‰°åŠ¨
        :param epsilon: æ‰°åŠ¨æƒé‡
        :param emb_name: éœ€è¦æ³¨å…¥æ‰°åŠ¨çš„ Embedding å±‚åç§°
        """
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                # å¤‡ä»½å½“å‰å‚æ•°
                self.backup[name] = param.data.clone()
                # è®¡ç®—æ¢¯åº¦èŒƒæ•°
                norm = torch.norm(param.grad)
                if norm != 0 and not torch.isnan(norm):
                    # è®¡ç®—æ‰°åŠ¨å¹¶å åŠ 
                    r_at = epsilon * param.grad / norm
                    param.data.add_(r_at)

    def restore(self, emb_name='word_embeddings'):
        """
        æ¢å¤è¢«æ‰°åŠ¨ä¹‹å‰çš„å‚æ•°
        """
        for name, param in self.model.named_parameters():
            if param.requires_grad and emb_name in name:
                assert name in self.backup
                param.data = self.backup[name]
        self.backup = {}

def multilabel_categorical_crossentropy(y_pred, y_true):
    """
    y_pred: [..., num_classes]
    y_true: [..., num_classes] (0 or 1)
    """
    # è¿™ä¸€æ­¥èƒ½ä¿è¯ä¸ç®¡ä½ æ˜¯ 2 ç»´è¿˜æ˜¯ 4 ç»´ï¼Œé€»è¾‘éƒ½èƒ½é—­çŽ¯
    shape = y_pred.shape
    y_pred = y_pred.reshape(-1, shape[-1])
    y_true = y_true.reshape(-1, shape[-1])
    
    y_pred = (1 - 2 * y_true) * y_pred
    y_pred_neg = y_pred - y_true * 1e12
    y_pred_pos = y_pred - (1 - y_true) * 1e12
    
    zeros = torch.zeros_like(y_pred[:, :1])
    y_pred_neg = torch.cat([y_pred_neg, zeros], dim=-1)
    y_pred_pos = torch.cat([y_pred_pos, zeros], dim=-1)
    
    # è¿”å›žæ¯ä¸ªæ ·æœ¬çš„ loss ä¹‹å’Œï¼Œå†å–å‡å€¼
    return (torch.logsumexp(y_pred_neg, dim=-1) + torch.logsumexp(y_pred_pos, dim=-1)).mean()

def evaluate(model, data_loader, device, id2rel, threshold=0.0):
    model.eval()
    X, Y, Z = 1e-10, 1e-10, 1e-10
    total_loss, total_ent_loss, total_head_loss, total_tail_loss = 0, 0, 0, 0
    
    with torch.no_grad():
        for batch in data_loader:
            ids, mask, y_ent, y_head, y_tail, texts, raw_spos, offsets = batch
            ids, mask, y_ent, y_head, y_tail = [x.to(device) for x in [ids, mask, y_ent, y_head, y_tail]]
            
            p_ent, p_head, p_tail = model(ids, mask)
            
            # è®¡ç®—ç»†åˆ† Loss
            l_ent = multilabel_categorical_crossentropy(p_ent, y_ent.unsqueeze(1))
            l_head = multilabel_categorical_crossentropy(p_head, y_head)
            l_tail = multilabel_categorical_crossentropy(p_tail, y_tail)
            
            batch_loss = (l_ent + l_head + l_tail) / 3
            total_ent_loss += l_ent.item()
            total_head_loss += l_head.item()
            total_tail_loss += l_tail.item()
            total_loss += batch_loss.item()

            for i in range(len(texts)):
                target = eval(raw_spos[i])
                target_set = set()
                for s in target:
                    target_set.add((s['h']['name'], tuple(s['h']['pos']), s['t']['name'], tuple(s['t']['pos']), s['relation']))
                
                # ðŸ’¡ ä½¿ç”¨ä¼ å…¥çš„é˜ˆå€¼è¿›è¡Œç¡¬åˆ¤å®š
                ent_matrix = p_ent[i, 0].cpu().numpy() > threshold
                head_matrix = p_head[i].cpu().numpy() > threshold
                tail_matrix = p_tail[i].cpu().numpy() > threshold
                current_offset = offsets[i].cpu().numpy()
                
                entities = {}
                for s, e in zip(*np.where(ent_matrix)):
                    start_char, end_char = int(current_offset[s][0]), int(current_offset[e][1])
                    name = texts[i][start_char: end_char]
                    if name.strip(): entities[(s, e)] = (name, [start_char, end_char])
                
                pred_set = set()
                # éåŽ†æ‰€æœ‰å·²è¯†åˆ«å‡ºçš„å®žä½“å¯¹ï¼Œæ£€æŸ¥å®ƒä»¬æ˜¯å¦å­˜åœ¨æŒ‡å®šçš„æŸç§å…³ç³»
                for (sh, eh), sub_info in entities.items():
                    for (so, eo), obj_info in entities.items():
                        # éåŽ†æ¯ä¸€ç§å…³ç³»ç±»åž‹
                        for rel_id in range(len(id2rel)):
                            # å¦‚æžœ head_matrix æ ‡è®°äº†ä¸»å®¢ä½“çš„èµ·å§‹ç‚¹ï¼Œä¸” tail_matrix æ ‡è®°äº†ä¸»å®¢ä½“çš„ç»“æŸç‚¹
                            if head_matrix[rel_id, sh, so] and tail_matrix[rel_id, eh, eo]:
                                pred_set.add((
                                    sub_info[0],        # ä¸»ä½“å
                                    tuple(sub_info[1]), # ä¸»ä½“ä½ç½® [start, end]
                                    obj_info[0],        # å®¢ä½“å
                                    tuple(obj_info[1]), # å®¢ä½“ä½ç½® [start, end]
                                    id2rel[rel_id]      # å…³ç³»ç±»åž‹
                                ))
                
                # ðŸ’¡ æ ¸å¿ƒè®¡æ•°é€»è¾‘
                X += len(pred_set & target_set) # é¢„æµ‹å¯¹çš„ (TP)
                Y += len(pred_set)              # é¢„æµ‹å‡ºçš„æ€»é‡ (TP + FP)
                Z += len(target_set)             # æ ·æœ¬çœŸå®žæ€»é‡ (TP + FN)

    num_batches = len(data_loader)
    
    # ðŸ’¡ è®¡ç®—æœ€ç»ˆæŒ‡æ ‡
    precision = X / Y
    recall = X / Z
    f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0
    
    metrics = {
        "loss": total_loss / num_batches,
        "ent_loss": total_ent_loss / num_batches,
        "head_loss": total_head_loss / num_batches,
        "tail_loss": total_tail_loss / num_batches,
        "f1": f1,
        "precision": precision,
        "recall": recall
    }
    return metrics